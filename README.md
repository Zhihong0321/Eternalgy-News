# Eternalgy-News-AI

AI-powered news discovery, processing, and translation system.

## ğŸ¯ Overview

Eternalgy-News-AI is an intelligent news aggregation system that discovers, processes, and translates news articles using advanced AI models. The system automatically finds news URLs, extracts content, cleans and summarizes it, and translates it into multiple languages.

## âœ¨ Features

- ğŸ” **AI-Powered Search**: GPT-4o-mini-web-search for intelligent news discovery
- ğŸ¤– **Content Processing**: Automated scraping, cleaning, and point-form summarization
- ğŸŒ **Multi-Language**: Automatic translation to English, Chinese (Simplified), and Malay
- ğŸ—„ï¸ **PostgreSQL Storage**: Deduplication and persistent storage
- ğŸ“Š **Structured Output**: Strict JSON format for easy frontend integration
- ğŸ”„ **Automated Workflow**: End-to-end pipeline from discovery to storage

## ğŸ—ï¸ Architecture

```
Query Task â†’ GPT-4o-mini Search â†’ News URLs
    â†“
PostgreSQL (news_links table)
    â†“ [Deduplication]
AI Processor â†’ HTTP Scrape â†’ Extract Content
    â†“
GPT-5-nano â†’ Clean & Summarize (Point Form)
    â†“
GPT-5-nano â†’ Translate (EN, ZH, MS)
    â†“
PostgreSQL (processed_content table)
    â†“
[Frontend UI - Ready for Integration]
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- Docker & Docker Compose
- API keys for:
  - GPT-4o-mini-web-search
  - GPT-5-nano (or compatible OpenAI API)

### 1. Setup Database

```bash
docker-compose up -d
```

This starts PostgreSQL on port 5433.

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Configure Environment

```bash
cp .env.example .env
```

Edit `.env` with your API keys:
```bash
SEARCH_API_KEY=your-search-api-key
AI_API_KEY=your-ai-api-key
```

### 4. Initialize Database

```bash
python examples/00_init_database.py
```

### 5. Run Full Workflow

```bash
python examples/04_full_workflow.py
```

## ğŸ“ Project Structure

```
Eternalgy-News-AI/
â”œâ”€â”€ news_search/           # News discovery module
â”‚   â”œâ”€â”€ search_client.py   # GPT-4o-mini search client
â”‚   â”œâ”€â”€ search_module.py   # Search orchestration
â”‚   â”œâ”€â”€ database.py        # PostgreSQL operations
â”‚   â””â”€â”€ processor_worker.py # Link processing worker
â”‚
â”œâ”€â”€ ai_processing/         # Content processing module
â”‚   â”œâ”€â”€ processor_with_content.py  # Main processor
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ content_extractor.py   # HTTP scraping
â”‚   â”‚   â”œâ”€â”€ content_cleaner.py     # Content cleaning
â”‚   â”‚   â””â”€â”€ translator.py          # Multi-language translation
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ article.py     # Data models
â”‚
â”œâ”€â”€ examples/              # Usage examples
â”‚   â”œâ”€â”€ 01_create_query_task.py
â”‚   â”œâ”€â”€ 02_run_search.py
â”‚   â”œâ”€â”€ 03_process_links.py
â”‚   â””â”€â”€ 04_full_workflow.py
â”‚
â”œâ”€â”€ docs/                  # Documentation
â”‚   â”œâ”€â”€ WORKFLOW_GUIDE.md
â”‚   â”œâ”€â”€ API_DOCUMENTATION.md
â”‚   â””â”€â”€ DATABASE_SCHEMA.md
â”‚
â””â”€â”€ frontend/              # Frontend integration (coming soon)
```

## ğŸ“– Documentation

- [Workflow Guide](docs/WORKFLOW_GUIDE.md) - Complete workflow documentation
- [API Documentation](docs/API_DOCUMENTATION.md) - API reference
- [Database Schema](docs/DATABASE_SCHEMA.md) - Database structure
- [Architecture](docs/ARCHITECTURE.md) - System design

## ğŸ”§ Modules

### news_search

News discovery using GPT-4o-mini-web-search with PostgreSQL deduplication.

**Key Features**:
- AI-powered web search
- URL normalization and deduplication
- Query task management
- Automatic processing trigger

### ai_processing

Content extraction, cleaning, and multi-language translation.

**Key Features**:
- HTTP content scraping
- Readability-based extraction
- Point-form summarization
- 3-language translation (EN, ZH, MS)
- Language detection

## ğŸ’¾ Database Schema

### news_links
Stores discovered news URLs with deduplication.

```sql
- id (serial)
- url (text)
- url_hash (varchar) - for deduplication
- title (text)
- discovered_at (timestamp)
- source_task (varchar)
- status (varchar) - pending/processing/completed/failed
```

### processed_content
Stores cleaned and translated articles.

```sql
- id (serial)
- link_id (integer) - foreign key to news_links
- title (text)
- content (text) - point-form summary
- translated_content (text) - JSON with EN/ZH/MS
- metadata (jsonb)
```

### query_tasks
Manages reusable search queries.

```sql
- id (serial)
- task_name (varchar)
- prompt_template (text)
- is_active (boolean)
- last_run (timestamp)
```

## ğŸŒ Frontend Integration

The system outputs structured JSON ready for any frontend framework:

```json
{
  "id": 123,
  "title": "Original title",
  "title_en": "English title",
  "title_zh": "ä¸­æ–‡æ ‡é¢˜",
  "title_ms": "Tajuk Melayu",
  "content": "â€¢ Point 1\nâ€¢ Point 2\nâ€¢ Point 3",
  "url": "https://example.com/article",
  "detected_language": "en",
  "processed_at": "2025-11-18T10:30:00",
  "metadata": {
    "source": "example.com",
    "word_count": 500
  }
}
```

**Coming Soon**: REST API and frontend template integration.

## ğŸ”„ Workflow Examples

### Create a Query Task

```python
from news_search import Database

db = Database()
db.create_query_task(
    task_name="tech_news",
    prompt_template="Find latest technology news URLs. Return as JSON array."
)
```

### Run Search and Process

```python
from news_search import NewsSearchModule, ProcessorWorker

# Initialize
search = NewsSearchModule()
processor = ProcessorWorker()

# Run search
result = search.run_task("tech_news")
print(f"Found {result['new_links']} new articles")

# Process articles
processor.process_pending_links(limit=10)
```

### Query Processed News

```python
from news_search import Database

db = Database()
content = db.get_processed_content(link_id=123)

print(f"Title (EN): {content['title_en']}")
print(f"Title (ZH): {content['title_zh']}")
print(f"Title (MS): {content['title_ms']}")
print(f"Content: {content['content']}")
```

## ğŸ› ï¸ Configuration

### Environment Variables

See `.env.example` for all configuration options:

- **Search API**: `SEARCH_API_URL`, `SEARCH_API_KEY`, `SEARCH_MODEL`
- **AI Processing**: `AI_API_URL`, `AI_API_KEY`, `AI_MODEL`
- **Database**: `DB_HOST`, `DB_PORT`, `DB_NAME`, `DB_USER`, `DB_PASSWORD`
- **Processing**: `AUTO_PROCESS_AFTER_SEARCH`, `MAX_CONCURRENT_DOMAINS`

## ğŸ§ª Testing

```bash
# Test search module
python tests/test_search.py

# Test AI processor
python tests/test_processor.py

# Test database
python tests/test_database.py
```

## ğŸ“Š Monitoring

Check processing status:

```python
from news_search import Database

db = Database()
stats = db.get_statistics()

print(f"Total links: {stats['links']['total_links']}")
print(f"Pending: {stats['links']['pending']}")
print(f"Completed: {stats['links']['completed']}")
print(f"Failed: {stats['links']['failed']}")
```

## ğŸ¤ Contributing

This is a private project. For questions or issues, contact the development team.

## ğŸ“„ License

Proprietary - All rights reserved.

## ğŸ”— Related Projects

- [TrendRadar](https://github.com/sansan0/TrendRadar) - Original inspiration (now independent)

---

**Version**: 1.0.0  
**Last Updated**: 2025-11-18  
**Status**: Production Ready (Backend) | Frontend Integration Pending
